# Optimize ML Models and Deploy Human-in-the-Loop Pipelines

Welcome to the GitHub repository for the third course in the Practical Data Science Specialization. This course is designed to enhance your skills in performance optimization and cost management for machine learning models using Amazon SageMaker and other AWS services. Here, you'll learn to fine-tune models, deploy them in a competitive environment, and incorporate human judgment into the machine learning workflow.

## What You'll Learn
In this course, you will delve into advanced techniques for improving the performance of your models and reducing operational costs:

**Hyper-parameter Tuning:** Use Amazon SageMaker Hyper-parameter Tuning (HPT) to automatically optimize the accuracy of your text classifiers.
**A/B Testing:** Deploy model candidates into a real-time A/B testing environment to evaluate their prediction performance. Learn how to automatically scale the more successful model using Amazon SageMaker Hosting.
**Human-in-the-Loop:** Integrate Amazon Augmented AI and Amazon SageMaker Ground Truth to create a human-in-the-loop pipeline. This pipeline will address misclassified predictions and help generate valuable new training data.

## Course Structure

This course covers:
**Model Tuning:** Apply hyper-parameter tuning to enhance model performance.
**Model Deployment and Testing:** Deploy models for A/B testing and learn techniques for scaling the best performing model.
**Human-in-the-Loop Systems:** Set up systems to incorporate human intelligence into the ML lifecycle, improving model accuracy and generating new data.

## Target Audience

This course is ideal for:
Developers
Data scientists
Data analysts
Who are already familiar with Python and SQL, and are looking to leverage AWS services to build scalable machine learning solutions.

## Prerequisites

Before starting this course, you should have:
A working knowledge of Python and SQL.
An active AWS account to access SageMaker and related services.
A basic understanding of machine learning concepts.